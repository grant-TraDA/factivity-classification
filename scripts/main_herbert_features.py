import logging
import pandas as pd
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
from sklearn.metrics import classification_report
from sklearn.neural_network import MLPClassifier
from sklearn.compose import ColumnTransformer
from torch.utils.data import DataLoader
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline

from src.data import VeridicalDataset
from src.herbert_classfier import HerBERTClassifier


#MODEL = "models/herbert_13_1e-05_32_T PL"
MODEL = "models/herbert_10_1e-05_32_verb"
model = torch.load(MODEL)

BATCH_SIZE = 62
LR = 0.001
text_col= 'verb'# 'T PL' #
y_col='GOLD <T,H>'


DIR_PROJECT = Path(".").resolve()
DIR_DATA = DIR_PROJECT.joinpath("data/17_10_2021")

logging.basicConfig(level=logging.INFO, filename=Path(".").resolve().joinpath(f'log/herbert_feat_{text_col}.log'))
logging.info(f"DATE: {datetime.today().strftime('%Y-%m-%d-%H:%M:%S')}")
logging.info(f"batch size: {BATCH_SIZE}, lr: {LR}, model: {MODEL}")

train = pd.read_csv(DIR_DATA.joinpath("train_data.csv"))
train_ = [{'text': row[text_col], 'label': row[y_col]} for _, row in train.iterrows()]
test = pd.read_csv(DIR_DATA.joinpath("test_data.csv"))
test_ = [{'text': row[text_col], 'label': row[y_col]} for _, row in test.iterrows()]
dev = pd.read_csv(DIR_DATA.joinpath("dev_data.csv"))
dev_ = [{'text': row[text_col], 'label': row[y_col]} for _, row in dev.iterrows()]


labels_ = sorted(list(set([elem['label'] for elem in train_])))
label2id_ = {label: i for i, label in enumerate(labels_)}
id2label_ = {i:label for i, label in enumerate(labels_)}

train_dataloader = DataLoader(
    VeridicalDataset(train_, labels=labels_, label2id=label2id_, id2label=id2label_),
    shuffle=True,
    batch_size = BATCH_SIZE
)
test_dataloader = DataLoader(
    VeridicalDataset(test_, labels=labels_, label2id=label2id_, id2label=id2label_),
    shuffle=False,
    batch_size = BATCH_SIZE
)
dev_dataloader = DataLoader(
    VeridicalDataset(dev_, labels=labels_, label2id=label2id_, id2label=id2label_),
    shuffle=False,
    batch_size = BATCH_SIZE
)


def transform_prediction(dataloader, model):
    _, _, embeddings = model.predict(dataloader)
    embeddings = torch.cat(embeddings, 0).cpu().numpy()
    embeddings = pd.DataFrame(embeddings, columns=['E', 'C', 'N'])
    return embeddings


train_embeddings = transform_prediction(train_dataloader, model)
test_embeddings = transform_prediction(test_dataloader, model)

X_train = train.drop(labels=['GOLD <T,H>', 'verb', 'T PL','H PL'], axis=1)
X_test = test.drop(labels=['GOLD <T,H>', 'verb', 'T PL','H PL'], axis=1)
train = pd.concat([train['GOLD <T,H>'], X_train, train_embeddings], 1)
test = pd.concat([test['GOLD <T,H>'], X_test, test_embeddings], 1)

# Train model
classifier = Pipeline([
    ('model', MLPClassifier(random_state=123, learning_rate_init=LR, max_iter=4000, batch_size=BATCH_SIZE))
])
classifier.fit(train.drop(labels=['GOLD <T,H>'], axis=1), train['GOLD <T,H>'])

logging.info("\nTEST")
y_pred = classifier.predict(test.drop(labels=['GOLD <T,H>'], axis=1))
logging.info(classification_report(test['GOLD <T,H>'], y_pred, digits=4))
